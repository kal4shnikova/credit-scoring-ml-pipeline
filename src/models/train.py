"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MLflow Tracking
"""
import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import (
    roc_auc_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_curve
)
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import joblib
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
sys.path.append(str(Path(__file__).resolve().parents[1]))
from models.pipeline import create_pipeline, get_feature_lists


def load_data(data_dir: Path):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–∞—é—â–∏—Ö –∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        data_dir: –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
        
    Returns:
        X_train, X_test, y_train, y_test
    """
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    train_df = pd.read_csv(data_dir / 'train.csv')
    test_df = pd.read_csv(data_dir / 'test.csv')
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    X_train = train_df.drop(['default', 'ID'], axis=1, errors='ignore')
    y_train = train_df['default']
    
    X_test = test_df.drop(['default', 'ID'], axis=1, errors='ignore')
    y_test = test_df['default']
    
    print(f"   Train: {X_train.shape[0]} samples, {X_train.shape[1]} features")
    print(f"   Test: {X_test.shape[0]} samples, {X_test.shape[1]} features")
    print(f"   Class balance (train): {y_train.value_counts().to_dict()}")
    
    return X_train, X_test, y_train, y_test


def calculate_metrics(y_true, y_pred, y_pred_proba):
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
    
    Args:
        y_true: –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        y_pred: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        y_pred_proba: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    metrics = {
        'roc_auc': roc_auc_score(y_true, y_pred_proba),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1_score': f1_score(y_true, y_pred)
    }
    
    return metrics


def plot_roc_curve(y_true, y_pred_proba, save_path):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ROC-–∫—Ä–∏–≤–æ–π
    
    Args:
        y_true: –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        y_pred_proba: –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
    """
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
    auc = roc_auc_score(y_true, y_pred_proba)
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.3f})', linewidth=2)
    plt.plot([0, 1], [0, 1], 'k--', label='Random classifier')
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('ROC Curve', fontsize=14)
    plt.legend(loc='lower right', fontsize=10)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=100)
    plt.close()
    
    print(f"   üìä ROC curve saved to {save_path}")


def plot_confusion_matrix(y_true, y_pred, save_path):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
    
    Args:
        y_true: –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        y_pred: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
    """
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.title('Confusion Matrix', fontsize=14)
    plt.tight_layout()
    plt.savefig(save_path, dpi=100)
    plt.close()
    
    print(f"   üìä Confusion matrix saved to {save_path}")


def train_model(
    model_type: str = "gradient_boosting",
    use_grid_search: bool = True,
    experiment_name: str = "credit_default_prediction"
):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    
    Args:
        model_type: —Ç–∏–ø –º–æ–¥–µ–ª–∏ ("gradient_boosting" –∏–ª–∏ "logistic_regression")
        use_grid_search: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ GridSearchCV
        experiment_name: –∏–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ MLflow
    """
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π
    project_dir = Path(__file__).resolve().parents[2]
    data_dir = project_dir / 'data' / 'processed'
    models_dir = project_dir / 'models'
    models_dir.mkdir(exist_ok=True)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow
    mlflow.set_tracking_uri("file:" + str(project_dir / "mlruns"))
    mlflow.set_experiment(experiment_name)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = load_data(data_dir)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    numeric_features, categorical_features = get_feature_lists()
    
    # –°—Ç–∞—Ä—Ç MLflow run
    with mlflow.start_run(run_name=f"{model_type}_model"):
        
        print(f"\nüöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {model_type}")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        if model_type == "gradient_boosting":
            if use_grid_search:
                param_grid = {
                    'classifier__n_estimators': [50, 100, 200],
                    'classifier__learning_rate': [0.01, 0.1, 0.2],
                    'classifier__max_depth': [3, 5, 7]
                }
            else:
                model_params = {
                    'n_estimators': 100,
                    'learning_rate': 0.1,
                    'max_depth': 5
                }
        else:  # logistic_regression
            if use_grid_search:
                param_grid = {
                    'classifier__C': [0.01, 0.1, 1.0, 10.0],
                    'classifier__penalty': ['l1', 'l2'],
                    'classifier__solver': ['liblinear']
                }
            else:
                model_params = {
                    'C': 1.0,
                    'penalty': 'l2'
                }
        
        # –°–æ–∑–¥–∞–Ω–∏–µ pipeline
        if use_grid_search:
            base_pipeline = create_pipeline(
                numeric_features,
                categorical_features,
                model_type=model_type
            )
            
            print(f"üîç GridSearchCV —Å {len(param_grid)} –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
            pipeline = GridSearchCV(
                base_pipeline,
                param_grid,
                cv=3,
                scoring='roc_auc',
                n_jobs=-1,
                verbose=1
            )
        else:
            pipeline = create_pipeline(
                numeric_features,
                categorical_features,
                model_type=model_type,
                **model_params
            )
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("üèãÔ∏è –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        pipeline.fit(X_train, y_train)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ GridSearch)
        if use_grid_search:
            best_params = pipeline.best_params_
            print(f"‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_params}")
            mlflow.log_params(best_params)
            final_model = pipeline.best_estimator_
        else:
            mlflow.log_param("model_type", model_type)
            if model_type == "gradient_boosting":
                mlflow.log_params({
                    'n_estimators': model_params.get('n_estimators'),
                    'learning_rate': model_params.get('learning_rate'),
                    'max_depth': model_params.get('max_depth')
                })
            else:
                mlflow.log_params({
                    'C': model_params.get('C'),
                    'penalty': model_params.get('penalty')
                })
            final_model = pipeline
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        print("üéØ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
        y_pred = final_model.predict(X_test)
        y_pred_proba = final_model.predict_proba(X_test)[:, 1]
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        print("üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...")
        metrics = calculate_metrics(y_test, y_pred, y_pred_proba)
        
        # –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫
        print("\n" + "="*50)
        print("üìà –ú–ï–¢–†–ò–ö–ò –ú–û–î–ï–õ–ò:")
        print("="*50)
        for metric_name, metric_value in metrics.items():
            print(f"   {metric_name}: {metric_value:.4f}")
        print("="*50 + "\n")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ MLflow
        mlflow.log_metrics(metrics)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        plots_dir = models_dir / 'plots'
        plots_dir.mkdir(exist_ok=True)
        
        roc_curve_path = plots_dir / f'roc_curve_{model_type}.png'
        plot_roc_curve(y_test, y_pred_proba, roc_curve_path)
        mlflow.log_artifact(str(roc_curve_path))
        
        cm_path = plots_dir / f'confusion_matrix_{model_type}.png'
        plot_confusion_matrix(y_test, y_pred, cm_path)
        mlflow.log_artifact(str(cm_path))
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model_path = models_dir / f'credit_default_model_{model_type}.pkl'
        joblib.dump(final_model, model_path)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ MLflow
        mlflow.sklearn.log_model(final_model, "model")
        
        # Classification report
        print("\n" + classification_report(y_test, y_pred))
        
        print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üîó MLflow Run ID: {mlflow.active_run().info.run_id}")


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='Train credit default prediction model')
    parser.add_argument(
        '--model',
        type=str,
        default='gradient_boosting',
        choices=['gradient_boosting', 'logistic_regression'],
        help='Model type to train'
    )
    parser.add_argument(
        '--grid-search',
        action='store_true',
        help='Use GridSearchCV for hyperparameter tuning'
    )
    
    args = parser.parse_args()
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    train_model(
        model_type=args.model,
        use_grid_search=args.grid_search
    )


if __name__ == "__main__":
    main()
